I"$<p>이 게시글에서는 양자 정보를 배우기 앞서 통계역학적으로 필요한 개념들을 소개한다. 많은 내용을 함축적으로 담았기 때문에 양자 정보에 대해 어떤 흐름으로 가게 되는지 살펴보는 정도로 알아본다.</p>

<h2 id="맥스웰의-악마-maxwells-demon">맥스웰의 악마 (Maxwell’s Demon)</h2>
<p>James Clerk Maxwell은 ‘맥스웰의 악마(Maxwell’s Demon)’이라는 사고실험을 하나 만들었다. 그 과정은 다음과 같다.</p>

<hr />

<ol>
  <li>바깥과 에너지 교환이 없는 계를 하나 생각하자. 이 공간에는 균일하게 기체인 입자들이 존재한다.</li>
  <li>그 공간을 정확히 절반으로 나누되, 출입할 수 있는 문 같은 것이 있다고 하자.</li>
  <li>이제 ‘악마(Demon)’라는 존재가 있다고 가정하자. 이 존재는 앞서 언급한 문을 이리저리 여닫을 수 있다.</li>
  <li>악마는 조건이 주어짐에 따라 문을 여닫을 수 있다. 예를 들어, 두 개로 나누어진 공간에 기체를 0.3, 0.7의 비율로 나누거나, 만약 두 종류의 기체로 섞여있을 경우,
이리저리 문을 여닫으며 출입하는 입자를 조절한다.</li>
</ol>

<hr />

<p>여기서 주요하게 볼 문제점은 ‘악마가 조건이 주어짐에 따라 행동하는 것’이다. 물론 예시로 든 경우들이 물리적으로 말도 안되지만, 악마는 이러한 조건을 ‘어디선가 받아 알고 있어야 한다.’
즉, 이 과정에서 악마는 <strong>‘조건’이라는 정보</strong>를 가지고 행동을 취한다고 말할 수 있다.</p>

<h2 id="열역학적-엔트로피-thermodynamic-entropy">열역학적 엔트로피 (Thermodynamic Entropy)</h2>
<p>양자 정보에서 주로 사용하는 <strong>폰 노이만 엔트로피(von Neumann Entropy)</strong>를 보기 앞서 어떻게 발전해왔는지 짧게 살펴보도록 하자.</p>

<h3 id="열역학-1법칙의-또다른-표현">열역학 1법칙의 또다른 표현</h3>
<p>먼저 우리는 열역학 제 1법칙인 에너지 보존식을 다음과 같이 기술한다.</p>

\[dU = \delta{Q} + \delta{W}\]

<p>여기서 $\delta$는 불완전미분을 말한다. 클라우지우스 원리로부터 엔트로피 변화는 다음과 같이 정의되었다.</p>

\[dS = \frac{\delta{Q_{rev}}}{T} \geq \frac{\delta{Q}}{T}\]

<p>여기서 rev는 가역적인(Reversible)을 의미한다. 그리고 같은 양의 기체가 용기에 들어있고 등압(Isobaric Work) 조건에서 가역과정인 경우, 열원에서 열을 받아들임으로 기체가 외부에 하는 일은</p>

\[\delta{W}_{rev} = -pdV\]

<p>여기서 부호가 음수인 것은 계의 입장에서 바라봤을 때 외부에 일을 함으로 에너지를 잃기 때문이다. 이제 dU를 다음으로 바꿔쓰자.</p>

\[dU = \delta{Q_{rev}} + \delta{W_{rev}} = TdS - pdV\]

<p>따라서 에너지 보존식이 T,S,p,V의 함수로 만들어졌다.</p>

<h3 id="볼츠만-엔트로피-boltzman-entropy">볼츠만 엔트로피 (Boltzman Entropy)</h3>
<p>앞에서 얻은 또 다른 에너지 보존식의 표현을 이용할 것이다. 이 식을 전미분의 형태로 쓰게되면,</p>

\[dU = \frac{\partial{U}}{\partial{S}} \vert_V dS + \frac{\partial{U}}{\partial{V}} \vert_S dV\]

<p>그러면 온도에 대한 식을 다음과 같이</p>

\[T = \frac{\partial{U}}{\partial{S}} \vert_V\]

<p>나타낼 수 있고, 여기서 에르고딕 가설에 의한 온도의 정의는</p>

\[\frac{1}{k_BT} = \frac{d\ln\Omega}{dE}\]

<p>여기서 $\Omega$는 microstate의 총 상태수로 양의 정수값을 가진다. E는 그 상태수가 가지는 에너지이다.
두 식을 결합하면,</p>

\[\frac{\partial{S}}{\partial{U}} \vert_V = {k_B}\frac{d\ln\Omega}{dE}\]

<p>그러므로 볼츠만의 엔트로피 표현을 얻는다.</p>

\[S = k_B\ln\Omega\]

<h3 id="깁스-엔트로피-gibbs-entropy">깁스 엔트로피 (Gibbs Entropy)</h3>
<p>앞에서 살펴본 엔트로피는 사실 거시적으로 살펴본 것이다. 만약에 이 거시상태들에 각각 동등한 미시 상태(microstate)가 주어진다면 어떻게 생각해야 할까? 전체적인 엔트로피를 다음과 같이 쓰자.</p>

\[S_{tot} = S + S_{micro}\]

<p>여기서 $S_{micro}$가 추가된 것이 우리가 고려해줘야 할 미시 상태의 엔트로피이며, S가 그것을 고려하지 않았을 때 엔트로피이다.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>microstate에 대해서 상태 측정은 불가능 하지만, 간단한 통계를 이용하여 가늠할 수 있다. 먼저 모든 microstate의 합이 우리가 측정한 거시상태의 수와 동일해야 하므로, 이를 N 이라고 할 때</p>

\[N = \sum_i{n_i}\]

<p>$n_i$는 각 개별 상태이다. 그리고 이를 발견할 확률을 $P_i$라고 하면</p>

\[P_i = \frac{n_i}{\sum_i{n_i}} \qquad \sum_iP_i = 1\]

<p>볼츠만의 엔트로피 표현에서 $\Omega$는 거시상태에서 보는 microstate의 총 수로 여기서 N과 같기 때문에, Total Entropy를</p>

\[S_{tot} = k_B\ln{N}\]

<p>으로 쓰고, microstate에 대한 엔트로피는, 각 개별 상태 엔트로피의 평균으로 나타낸다.</p>

\[S_{micro} = \langle s_i \rangle = \sum_i P_is_i = \sum_i P_ik_B\ln{n_i}\]

<p>그러므로 microstate를 제외한 엔트로피 S를</p>

\[\begin{aligned}
S &amp;= S_{tot} - S_{micro} \\
&amp;= k_B\ln{N} - \sum_i P_ik_B\ln{n_i} \\
&amp;= k_B\sum_i P_i \left( \ln{N} - \ln{n_i} \right) \\
&amp;= k_B\sum_i P_i \ln\frac{N}{n_i} \\
&amp;= -k_B\sum_i P_i \ln\frac{n_i}{N} \\
&amp;= -k_B\sum_i P_i \ln{P_i}
\end{aligned}\]

<p>Gibbs entropy 표현은 우리가 실제 측정할 수 없는 microstate 를 제외하고 상태의 확률을 알려주는 역할을 해준다.</p>

<h2 id="정보이론적-엔트로피-informatic-entropy">정보이론적 엔트로피 (Informatic Entropy)</h2>
<h3 id="섀넌-엔트로피-shannon-entropy">섀넌 엔트로피 (Shannon Entropy)</h3>
<p>Claude Shannon은 어떤 정보량 Q에 대해 다음과 같이 정의하였다.</p>

\[Q = -k\log_2 P\]

<p>여기서 k는 양의 상수로, 대부분 1 로 둔다. 정보량에 음수가 붇는다는 것은, 어떤 문장에 담겨있는 확률 P가 작으면 정보 Q는 많아지는 의미를 가진다. Gibbs entropy 표현과 같이 맞추면</p>

\[S = \langle Q \rangle = -k\sum_iP_i\log_2P_i\]

<p>이를 Shannon entropy라고 한다. 엄밀하게 따지면, $k=k_B, log_2 \rightarrow \ln$을 만족해야 Gibbs Entropy가 된다.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 그러므로 정보이론에서의 엔트로피는 상수들이 의미하는 바가 열역학적 엔트로피와는 같지 않다.
하지만 다소 정확하지 않은 접근이지만, 표현할 수 있는 식의 형태가 비슷하다. 그러므로 정보이론과 통계역학을 연관 짓는데에는 도움이 된다.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></p>

<h3 id="폰-노이만-엔트로피-von-neumann-entropy">폰 노이만 엔트로피 (von Neumann Entropy)</h3>
<p>이제 폰 노이만 엔트로피에 대해서 알아볼 차례가 되었다. 폰 노이만 엔트로피는 확률이 있는 자리에 양자역학에서의 밀도 행렬(density matrix)로 대치가 된다. 즉 다시 말해,</p>

\[S = -Tr(\rho\ln\rho)\]

<p>여기서 자연로그로 바뀐것은 일반적으로는 양자계에서 밑이 2인 로그 보다는 자연로그를 사용하기가 용이하기 때문이다. 이 때 여기서 Density Matrix가 Pure State이면
<a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">행렬 고윳값 분해(대각화)</a>로</p>

\[S = -\sum_i^n\lambda_i\ln{\lambda_i}\]

<p>이 되며 앞의 Shannon Entropy와 유사한 형태를 띄게된다. 이것에 대한 자세한 계산과정과 설명은,
‘<a href="physics/Schmidt-Decomposition/#폰-노이만-엔트로피와-concurrence">슈미트 분해</a>’ 게시글을 참고할 수 있다.</p>

<h2 id="references">References</h2>
<ul>
  <li>Stephen J. Blundell, Concepts in Thermal Physics, CH14, CH15</li>
  <li>Wikipedia, <a href="https://en.wikipedia.org/wiki/Von_Neumann_entropy">von Neumann entropy</a></li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>여기서 의문점, 만약에 microstate까지 고려한다면 곱의 형태로 나타내야 하지 않을 까? 그것은 앞의 볼츠만의 엔트로피 표현에서, <strong>로그</strong>가 들어간다는 점을 생각해보면 된다. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>사실 로그가 다른 것은 그렇게 중요하진 않다. 로그의 관계식으로 밑만 바꾸면 되기 때문에. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>물론 열과 정보 사이의 관계는 Rolf Landauer가 제시한 란다우어의 원리라는 것으로 연결짓기도 한다. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET